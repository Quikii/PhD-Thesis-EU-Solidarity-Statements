{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "df = read_csv(\"/content/EUSpeech_translated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "#import coreferee\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm  # For progress display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Tokenization and key sentence extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab/english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Function to get rokenized words and sentences\n",
    "# ----------------------------\n",
    "def tokenize_text(df, text_col=\"text.en\", keywords=None):\n",
    "    \"\"\"\n",
    "    Filters rows containing any of the specified keywords in the text,\n",
    "    tokenizes the text into words, splits the text into sentences, and extracts\n",
    "    context sentences that contain the keywords along with their adjacent sentences.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with a text column.\n",
    "        text_col (str): Name of the column containing text (default \"text.en\").\n",
    "        keywords (list of str): List of keywords to filter the text. \n",
    "                                If None, defaults to the EU Eastern Partnership countries:\n",
    "                                [\"Armenia\", \"Azerbaijan\", \"Belarus\", \"Georgia\", \"Moldova\", \"Ukraine\"].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the DataFrame with new columns:\n",
    "            - 'tokenized': List of tokens from the text.\n",
    "            - 'sentences': List of sentences from the text.\n",
    "            - 'context': List of context strings (each is a snippet including the matching sentence \n",
    "                         plus its preceding and following sentences, if available).\n",
    "    \"\"\"\n",
    "    # Default keywords if not provided\n",
    "    if keywords is None:\n",
    "        keywords = [\"Armenia\", \"Azerbaijan\", \"Belarus\", \"Georgia\", \"Moldova\", \"Ukraine\"]\n",
    "\n",
    "    # Compile a regex pattern from the keywords for filtering text (case-insensitive)\n",
    "    regex_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(map(re.escape, keywords)))\n",
    "    pattern = re.compile(regex_pattern, re.IGNORECASE)\n",
    "\n",
    "    # Filter rows that contain any of the keywords\n",
    "    df_filtered = df[df[text_col].apply(lambda x: bool(pattern.search(x)))].copy()\n",
    "\n",
    "    # Tokenize the text into words and store in a new column 'tokenized'\n",
    "    df_filtered['tokenized'] = df_filtered[text_col].apply(word_tokenize)\n",
    "\n",
    "    # Split the text into sentences and store in a new column 'sentences'\n",
    "    df_filtered['sentences'] = df_filtered[text_col].apply(sent_tokenize)\n",
    "\n",
    "    def extract_context(sentences, keywords):\n",
    "        \"\"\"\n",
    "        Extract context sentences that contain any of the keywords along with their adjacent sentences.\n",
    "        \"\"\"\n",
    "        contexts = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # Check if the sentence contains any keyword (case-insensitive)\n",
    "            if any(re.search(r'\\b{}\\b'.format(re.escape(keyword)), sentence, re.IGNORECASE) for keyword in keywords):\n",
    "                # Get the previous sentence if exists, current, and the next sentence if exists\n",
    "                start = max(i - 1, 0)\n",
    "                end = min(i + 2, len(sentences))\n",
    "                context = \" \".join(sentences[start:end])\n",
    "                contexts.append(context)\n",
    "        return contexts\n",
    "\n",
    "    # Apply context extraction for each row based on its list of sentences\n",
    "    df_filtered['context'] = df_filtered['sentences'].apply(lambda sents: extract_context(sents, keywords))\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.0/en_coreference_web_trf-3.4.0a0-py3-none-any.whl#egg=en_coreference_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "!pip show spacy\n",
    "import spacy_experimental\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_corefs(doc):\n",
    "    \"\"\"\n",
    "    Returns a version of the document text where for each non-main coreferent mention,\n",
    "    the main mention is appended in square brackets.\n",
    "    \n",
    "    For example, if a mention is \"he\" and the main mention for that coreference chain is \"Fidel Castro\",\n",
    "    the function will change \"he\" to \"he [Fidel Castro]\".\n",
    "    \n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): A spaCy document processed with a coreference model that sets doc.spans.\n",
    "    \n",
    "    Returns:\n",
    "        str: The modified text with coreference annotations appended.\n",
    "    \"\"\"\n",
    "    resolved_text = doc.text\n",
    "    offset = 0\n",
    "    insertions = []\n",
    "    \n",
    "    # Iterate over each coreference chain in doc.spans.\n",
    "    for chain in doc.spans:\n",
    "        # Process only chains that begin with the expected prefix.\n",
    "        if not chain.startswith(\"coref_clusters\"):\n",
    "            continue\n",
    "        \n",
    "        # The first mention is the main reference.\n",
    "        main_text = doc.spans[chain][0].text\n",
    "        \n",
    "        # For every subsequent mention, we want to append the main text in brackets.\n",
    "        for idx, span in enumerate(doc.spans[chain]):\n",
    "            if idx > 0:\n",
    "                # Insert after the mention; use span.end_char as the insertion point.\n",
    "                insertions.append([span.end_char, f\" [{main_text}]\"])\n",
    "    \n",
    "    # Sort the insertions by the character index.\n",
    "    for pos, insertion in sorted(insertions, key=lambda x: x[0]):\n",
    "        resolved_text = resolved_text[:pos + offset] + insertion + resolved_text[pos + offset:]\n",
    "        offset += len(insertion)\n",
    "    \n",
    "    return resolved_text\n",
    "\n",
    "# Example usage:\n",
    "import spacy\n",
    "import spacy_experimental\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")\n",
    "text = \"\"\"Fidel Castro led a communist revolution that toppled the Cuban government in 1959, after which he declared himself prime minister. He held the title until 1976, when it was abolished and he became head of the Communist Party and president of the council of state and the council of ministers. With his health failing, Castro handed power to his brother, Ra√∫l, in 2006. He died in 2016.\"\"\"\n",
    "doc = nlp(text)\n",
    "resolved = append_corefs(doc)\n",
    "print(\"Original text:\")\n",
    "print(doc.text)\n",
    "print(\"\\nText with coreferences appended:\")\n",
    "print(resolved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Running Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Define Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Define processing function for each text entry\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Define processing function for each text entry\n",
    "# ----------------------------\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process a single text:\n",
    "      - Extract Named Entities, subject, and object from the original text.\n",
    "      - Resolve coreference using coreferee (via append_corefs) on the entire text.\n",
    "      \n",
    "    Returns:\n",
    "        tuple: (sentence_records, resolved_text)\n",
    "        \n",
    "        - sentence_records: A list of sentence-level dictionaries with keys:\n",
    "            \"sentence\", \"entities\", \"subject\", and \"object\".\n",
    "        - resolved_text: The entire text with coreference annotations appended.\n",
    "    \"\"\"\n",
    "    # Process the original text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentence_records = []\n",
    "    # Extract sentence-level data from the original text\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if not sent_text:\n",
    "            continue\n",
    "        \n",
    "        sent_doc = nlp(sent_text)\n",
    "        entities = [(ent.text, ent.label_) for ent in sent_doc.ents]\n",
    "        \n",
    "        subject = None\n",
    "        obj = None\n",
    "        for token in sent_doc:\n",
    "            if token.dep_ in (\"nsubj\", \"nsubjpass\") and subject is None:\n",
    "                subject = token.text\n",
    "            if token.dep_ in (\"dobj\", \"pobj\") and obj is None:\n",
    "                obj = token.text\n",
    "        \n",
    "        sentence_records.append({\n",
    "            \"sentence\": sent_text,\n",
    "            \"entities\": entities,\n",
    "            \"subject\": subject,\n",
    "            \"object\": obj\n",
    "        })\n",
    "    \n",
    "    # Apply coreference resolution using append_corefs\n",
    "    resolved_text = append_corefs(doc)\n",
    "    \n",
    "    return sentence_records, resolved_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Run the Parallelized Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Define processing function for each text entry\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Define processing function for each text entry\n",
    "# ----------------------------\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process a single text:\n",
    "      - Extract Named Entities, subject, and object from the original text.\n",
    "      - Resolve coreference using coreferee (via append_corefs) on the entire text.\n",
    "      \n",
    "    Returns:\n",
    "        tuple: (sentence_records, resolved_text)\n",
    "        \n",
    "        - sentence_records: A list of sentence-level dictionaries with keys:\n",
    "            \"sentence\", \"entities\", \"subject\", and \"object\".\n",
    "        - resolved_text: The entire text with coreference annotations appended.\n",
    "    \"\"\"\n",
    "    # Process the original text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentence_records = []\n",
    "    # Extract sentence-level data from the original text\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if not sent_text:\n",
    "            continue\n",
    "        \n",
    "        sent_doc = nlp(sent_text)\n",
    "        entities = [(ent.text, ent.label_) for ent in sent_doc.ents]\n",
    "        \n",
    "        subject = None\n",
    "        obj = None\n",
    "        for token in sent_doc:\n",
    "            if token.dep_ in (\"nsubj\", \"nsubjpass\") and subject is None:\n",
    "                subject = token.text\n",
    "            if token.dep_ in (\"dobj\", \"pobj\") and obj is None:\n",
    "                obj = token.text\n",
    "        \n",
    "        sentence_records.append({\n",
    "            \"sentence\": sent_text,\n",
    "            \"entities\": entities,\n",
    "            \"subject\": subject,\n",
    "            \"object\": obj\n",
    "        })\n",
    "    \n",
    "    # Apply coreference resolution using append_corefs\n",
    "    resolved_text = append_corefs(doc)\n",
    "    \n",
    "    return sentence_records, resolved_text\n",
    "\n",
    "# ----------------------------\n",
    "# Create a DataFrame of sentence-level records\n",
    "# ----------------------------\n",
    "\n",
    "df_sentences = pd.DataFrame(results_sentences)\n",
    "df_sentences.to_csv('df_sentences_test_EUSpeech.csv', index=False)\n",
    "print(df_sentences.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
