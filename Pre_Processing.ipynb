{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "df = read_csv(\"/content/EUSpeech_translated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "#import coreferee\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm  # For progress display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Tokenization and key sentence extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab/english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Function to get rokenized words and sentences\n",
    "# ----------------------------\n",
    "def tokenize_text(df, text_col=\"text.en\", keywords=None):\n",
    "    \"\"\"\n",
    "    Filters rows containing any of the specified keywords in the text,\n",
    "    tokenizes the text into words, splits the text into sentences, extracts\n",
    "    context sentences that contain the keywords along with their adjacent sentences,\n",
    "    and extracts sentence-level records (sentence text, entities, subject, object)\n",
    "    using spaCy, all within this single function.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with a text column.\n",
    "        text_col (str): Name of the column containing text (default \"text.en\").\n",
    "        keywords (list of str): List of keywords to filter the text. \n",
    "                                If None, defaults to:\n",
    "                                [\"Armenia\", \"Azerbaijan\", \"Belarus\", \"Georgia\", \"Moldova\", \"Ukraine\"].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the DataFrame with new columns:\n",
    "            - 'tokenized': List of tokens from the text.\n",
    "            - 'sentences': List of sentences from the text.\n",
    "            - 'context': List of context strings (each is a snippet including the matching sentence \n",
    "                         plus its preceding and following sentences, if available).\n",
    "            - 'sentence_records': List of dictionaries with sentence-level data (sentence, entities, subject, object).\n",
    "            - 'matched_keyword': The matched keyword(s) found in the text.\n",
    "    \"\"\"\n",
    "    # Default keywords if not provided\n",
    "    if keywords is None:\n",
    "        keywords = [\"Armenia\", \"Azerbaijan\", \"Belarus\", \"Georgia\", \"Moldova\", \"Ukraine\"]\n",
    "\n",
    "    # Compile a regex pattern from the keywords for filtering text (case-insensitive)\n",
    "    regex_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(map(re.escape, keywords)))\n",
    "    pattern = re.compile(regex_pattern, re.IGNORECASE)\n",
    "\n",
    "    # Pre-filter rows that contain any of the keywords in the text\n",
    "    df_filtered = df[df[text_col].apply(lambda x: bool(pattern.search(x)) if isinstance(x, str) else False)].copy()\n",
    "\n",
    "    # Add a new column with the matched keyword(s)\n",
    "    def extract_keywords(text):\n",
    "        if isinstance(text, str):\n",
    "            matches = pattern.findall(text)\n",
    "            return \", \".join(sorted(set(matches))) if matches else None\n",
    "        return None\n",
    "\n",
    "    df_filtered[\"matched_keyword\"] = df_filtered[text_col].apply(extract_keywords)\n",
    "\n",
    "    # Tokenize the text into words and store in a new column 'tokenized'\n",
    "    df_filtered['tokenized'] = df_filtered[text_col].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "    # Split the text into sentences and store in a new column 'sentences'\n",
    "    df_filtered['sentences'] = df_filtered[text_col].apply(lambda x: sent_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "    # Extract context for each row based on its list of sentences\n",
    "    def extract_context(sentences):\n",
    "        contexts = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # If the sentence contains any keyword (case-insensitive)\n",
    "            if any(re.search(r'\\b{}\\b'.format(re.escape(keyword)), sentence, re.IGNORECASE) for keyword in keywords):\n",
    "                start = max(i - 1, 0)\n",
    "                end = min(i + 2, len(sentences))\n",
    "                context = \" \".join(sentences[start:end])\n",
    "                contexts.append(context)\n",
    "        return contexts\n",
    "\n",
    "    df_filtered['context'] = df_filtered['sentences'].apply(extract_context)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.0/en_coreference_web_trf-3.4.0a0-py3-none-any.whl#egg=en_coreference_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "!pip show spacy\n",
    "import spacy_experimental\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_corefs(text):\n",
    "    \"\"\"\n",
    "    Returns a version of the document text where for each non-main coreferent mention,\n",
    "    the main mention is appended in square brackets.\n",
    "    \n",
    "    For example, if a mention is \"he\" and the main mention for that coreference chain is \"Fidel Castro\",\n",
    "    the function will change \"he\" to \"he [Fidel Castro]\".\n",
    "    \n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): A spaCy document processed with a coreference model that sets doc.spans.\n",
    "    \n",
    "    Returns:\n",
    "        str: The modified text with coreference annotations appended.\n",
    "    \"\"\"\n",
    "    resolved_text = text\n",
    "    offset = 0\n",
    "    insertions = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Iterate over each coreference chain in doc.spans.\n",
    "    for chain in doc.spans:\n",
    "        # Process only chains that begin with the expected prefix.\n",
    "        if not chain.startswith(\"coref_clusters\"):\n",
    "            continue\n",
    "        \n",
    "        # The first mention is the main reference.\n",
    "        main_text = doc.spans[chain][0].text\n",
    "        \n",
    "        # For every subsequent mention, we want to append the main text in brackets.\n",
    "        for idx, span in enumerate(doc.spans[chain]):\n",
    "            if idx > 0:\n",
    "                # Insert after the mention; use span.end_char as the insertion point.\n",
    "                insertions.append([span.end_char, f\" [{main_text}]\"])\n",
    "    \n",
    "    # Sort the insertions by the character index.\n",
    "    for pos, insertion in sorted(insertions, key=lambda x: x[0]):\n",
    "        resolved_text = resolved_text[:pos + offset] + insertion + resolved_text[pos + offset:]\n",
    "        offset += len(insertion)\n",
    "    \n",
    "    return resolved_text\n",
    "\n",
    "# Example usage:\n",
    "import spacy\n",
    "import spacy_experimental\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")\n",
    "text = \"\"\"Fidel Castro led a communist revolution that toppled the Cuban government in 1959, after which he declared himself prime minister. He held the title until 1976, when it was abolished and he became head of the Communist Party and president of the council of state and the council of ministers. With his health failing, Castro handed power to his brother, Ra√∫l, in 2006. He died in 2016.\"\"\"\n",
    "resolved = append_corefs(text)\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\nText with coreferences appended:\")\n",
    "print(resolved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Running Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Define Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Define De-Bugged Processing function \n",
    "# ----------------------------\n",
    "\n",
    "def debug_process_text(df):\n",
    "    try:\n",
    "        print(f\"[DEBUG] Processing text (first 50 chars): {df[:50]}...\")\n",
    "        df_filtered = tokenize_text(df)\n",
    "        print(f\"[DEBUG] Finished processing text (first 50 chars): {df[:50]}...\")\n",
    "        return df_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception while processing text (first 50 chars): {df[:50]}... : {e}\")\n",
    "        raise\n",
    "    if False:\n",
    "        try:\n",
    "            print(f\"[DEBUG] Processing text (first 50 chars): {df[:50]}...\")\n",
    "            df_filtered[\"resolved_text\"] = df_filtered[\"text.en\"].apply(\n",
    "                lambda t: append_corefs(nlp(t)) if isinstance(t, str) else t\n",
    "            )\n",
    "            print(f\"[DEBUG] Finished processing text (first 50 chars): {df[:50]}...\")\n",
    "            return df_filtered\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Exception while processing text (first 50 chars): {df[:50]}... : {e}\")\n",
    "            raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Run the Parallelized Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# MAIN WORKFLOW\n",
    "# ----------------------------\n",
    "# Parallelize processing for all texts in df_prefiltered (assumed to have a \"text.en\" column).\n",
    "results_dfs = []\n",
    "max_workers = 30\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit one task per row.\n",
    "    futures = {executor.submit(debug_process_text, df.iloc[[i]]): i \n",
    "               for i in range(len(df))}\n",
    "    print(f\"[DEBUG] Submitted {len(futures)} tasks to the executor.\")\n",
    "    \n",
    "    for future in tqdm(concurrent.futures.as_completed(futures),\n",
    "                       total=len(futures),\n",
    "                       desc=\"Processing texts\"):\n",
    "        try:\n",
    "            result_df = future.result()  # Each result is an enriched DataFrame for one row.\n",
    "            results_dfs.append(result_df)\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] Error processing a text: {exc}\")\n",
    "\n",
    "# Concatenate all single-row DataFrames into one final DataFrame.\n",
    "df_filtered_all = pd.concat(results_dfs, ignore_index=True)\n",
    "print(\"[DEBUG] Final enriched DataFrame:\")\n",
    "print(df_filtered_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_all.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
