{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZVLKxqov8KE"
      },
      "source": [
        "0. Data and Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hzemCe5v8KH"
      },
      "source": [
        "0.1 Preprocessed Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7GuNHfsjv8KI",
        "outputId": "9da8137b-f597-46b8-933d-f6dd213ace38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/qhaskovec/OneDrive/GitHub/PhD-Thesis-EU-Solidarity-Statements/EUSpeech_translated_preprocessed.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b6c333df1e39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/qhaskovec/OneDrive/GitHub/PhD-Thesis-EU-Solidarity-Statements/EUSpeech_translated_preprocessed.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/qhaskovec/OneDrive/GitHub/PhD-Thesis-EU-Solidarity-Statements/EUSpeech_translated_preprocessed.csv'"
          ]
        }
      ],
      "source": [
        "from pandas import read_csv\n",
        "df = read_csv(\"C:/Users/qhaskovec/OneDrive/GitHub/PhD-Thesis-EU-Solidarity-Statements/EUSpeech_translated_preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUNfXn7sv8KJ"
      },
      "source": [
        "0.2 Libraries (Especially Unsloth Download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTa3Dih-v8KJ"
      },
      "outputs": [],
      "source": [
        "#capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpOW3Y3Xv8KL"
      },
      "outputs": [],
      "source": [
        "!pip install xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuH8L4Xbv8KL"
      },
      "source": [
        "1. Initialize Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BohSmoPwv8KM"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2000 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model_Llama3.2_3B\", # Pretrained unsloth model\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktFaNzl5v8KN"
      },
      "source": [
        "2. Test Whether it Works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWsxj3Jgv8KN"
      },
      "outputs": [],
      "source": [
        "instruction = (\n",
        "    \"You are an expert researcher in political science. \"\n",
        "    \"Analyze the following text and determine which of the following solidarity categories best describe it: \"\n",
        "    \"Sentence to be analyzed: We as a Community of European States must hold together in times of crisis\"\n",
        "    \"POSSIBLE LABELS:\"\n",
        "    \"1. National Solidarity\"\n",
        "    \"2. European-Level Solidarity,\"\n",
        "    \"3. Outside-Europe Solidarity. \"\n",
        "    \"Decide which label is most suitable and provide the output in a JSON format like this:\"\n",
        "    \"{\\\"label\\\": \\\"<Insert one label: National Solidarity, European-Level Solidarity, or Outside-Europe Solidarity>\\\"}\"\n",
        "    \"ONLY WRITE IN THE JSON FORMAT. GENERATE NOTHING ELSE.\"\n",
        ")\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": instruction},\n",
        "    {\"role\": \"user\", \"content\": \"We as a Community of European States must hold together in times of crisis\"}\n",
        "]\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 300,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_xZFCKov8KO"
      },
      "source": [
        "3. Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mk6HkHVv8KP"
      },
      "source": [
        "3.1 Extract Solidarity Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A_aKX9sv8KR"
      },
      "outputs": [],
      "source": [
        "def code_solidarity_label(input_text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    This function instructs the model, an expert political scientist, to analyze a political statement\n",
        "    and decide if it expresses any indication of solidarity towards another actor.\n",
        "    The output is a JSON array with a single object containing two keys:\n",
        "      - \"Solidarity\": \"Solidarity\" if the statement shows explicit support, assistance, or positive alignment towards another party;\n",
        "                        \"No Solidarity\" otherwise.\n",
        "      - \"Justification\": A brief explanation (one or two sentences) justifying the label, referring to specific phrases if applicable.\n",
        "\n",
        "    Parameters:\n",
        "        input_text (str): The text to analyze.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's generated response as a structured JSON array.\n",
        "    \"\"\"\n",
        "\n",
        "    instruction = (\n",
        "        \"You are an expert political scientist specializing in discourse analysis. \"\n",
        "        \"Your task is to analyze the following statement and determine whether it expresses any indication of solidarity towards another actor. \"\n",
        "        \"For your response, return a JSON array with a single object containing two keys: 'Solidarity' and 'Justification'. \"\n",
        "        \"The 'Solidarity' key should have the value 'Solidarity' if the statement shows support, or 'No Solidarity' if it does not. \"\n",
        "        \"The 'Justification' key should provide a brief explanation (one or two sentences) citing specific phrases or elements that justify your label. \\n\\n\"\n",
        "        \"For example, if the input is:\\n\"\n",
        "        \"\\\"I stand with you in these trying times, offering all my support,\\\" \\n\"\n",
        "        \"then the output should be:\\n\"\n",
        "        \"[\\n\"\n",
        "        \"  {\\n\"\n",
        "        \"    \\\"Solidarity\\\": \\\"Solidarity\\\",\\n\"\n",
        "        \"    \\\"Justification\\\": \\\"The statement explicitly offers support and solidarity through phrases like 'I stand with you' and 'offering all my support'.\\\"\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"]\\n\\n\"\n",
        "        \"If the input is:\\n\"\n",
        "        \"\\\"I am indifferent to your situation,\\\" \\n\"\n",
        "        \"then the output should be:\\n\"\n",
        "        \"[\\n\"\n",
        "        \"  {\\n\"\n",
        "        \"    \\\"Solidarity\\\": \\\"No Solidarity\\\",\\n\"\n",
        "        \"    \\\"Justification\\\": \\\"The statement does not express any form of support or solidarity, as indicated by the word 'indifferent'.\\\"\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"]\\n\\n\"\n",
        "        \"Now, analyze the following text:\\n\"\n",
        "        f\"{input_text}\"\n",
        "    )\n",
        "\n",
        "    # Build the prompt (here we're combining the instruction and the input text)\n",
        "    prompt = instruction\n",
        "\n",
        "    # Create the text-generation pipeline with a controlled temperature setting\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
        "    output = pipe(prompt, max_length=400)\n",
        "    generated_text = output[0]['generated_text'].strip()\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIy4aSd5v8KT"
      },
      "source": [
        "3.2 Extract Solidarity Speech Acts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEwP_-nIv8KU"
      },
      "outputs": [],
      "source": [
        "def code_solidarity_speech_acts(input_text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Function that instructs the model to analyze political text and extract solidarity-related speech acts,\n",
        "    returning a structured JSON array with the identified information.\n",
        "\n",
        "    Parameters:\n",
        "        input_text (str): The political text to be analyzed.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The coded response from the model as a structured JSON array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Optimized instruction for extracting solidarity-related speech acts with structured output\n",
        "    instruction = (\n",
        "        \"You are an expert in political science and computational discourse analysis. \"\n",
        "        \"Your task is to analyze the following political text to identify and code all instances of solidarity-related speech acts. \"\n",
        "        \"A valid solidarity-related speech act must meet these criteria: \"\n",
        "        \"1. Transitive Structure: The utterance must contain a transitive verb structure linking a subject (the provider, typically the speaker) to a direct object (the recipient). \"\n",
        "        \"2. Positive Orientation: The subject expresses a positive orientation (they offer some form of assistance or help, even if verbal) toward the recipient, who is understood to be in a comparatively difficult situation. \"\n",
        "        \"3. Illocutionary Type: The speech act must clearly fall into one of the following categories: \"\n",
        "        \"   - Solidarity Expressives: Utterances that communicate emotional support or compassion without calling for specific action. \"\n",
        "        \"   - Solidarity Directives: Utterances that demand, request, or call on others to act in support of the recipient. \"\n",
        "        \"   - Solidarity Commissives: Utterances in which the speaker commits themselves to take future actions to support the recipient. \"\n",
        "        \"For each solidarity-related speech act identified in the text, extract and return the following information: \"\n",
        "        \"   - SpeechActType: Label the act as 'Expressive,' 'Directive,' or 'Commissive.' \"\n",
        "        \"   - Provider: Identify the actor (usually the speaker) performing the solidarity action. \"\n",
        "        \"   - Recipient: Identify the actor (direct object) who is in need and toward whom the positive orientation is directed. \"\n",
        "        \"   - PropositionalContent: Summarize the core message or demand embedded in the speech act. \"\n",
        "        \"If a sentence does not contain a solidarity-related speech act or lacks the required transitive structure, do not return anything. \"\n",
        "        \"Return your analysis as a structured JSON array where each element represents an identified speech act with the keys \"\n",
        "        \"'SpeechActType,' 'Provider,' 'Recipient,' and 'PropositionalContent.' \"\n",
        "        \"Do not include any additional commentary or text beyond this structured output. \"\n",
        "        \"For example, your output should look like this:\\n\"\n",
        "        \"[\\n\"\n",
        "        \"  {\\n\"\n",
        "        \"    \\\"SpeechActType\\\": \\\"\\\",\\n\"\n",
        "        \"    \\\"Provider\\\": \\\"\\\",\\n\"\n",
        "        \"    \\\"Recipient\\\": \\\"\\\",\\n\"\n",
        "        \"    \\\"PropositionalContent\\\": \\\"\\\"\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"]\\n\"\n",
        "        \"Text to analyze:\\n\"\n",
        "        f\"{input_text}\"\n",
        "    )\n",
        "\n",
        "    # Structuring the message with system and user roles\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": instruction}\n",
        "    ]\n",
        "\n",
        "    # Creating the text-generation pipeline with a deterministic setting\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
        "    output = pipe(messages)\n",
        "\n",
        "    # Return the generated response containing the structured JSON analysis\n",
        "    return output[0]['generated_text']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2_53lmEv8KW"
      },
      "source": [
        "4. Run the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsuYosJNv8KX"
      },
      "outputs": [],
      "source": [
        "# Define a helper function to call the model and parse the JSON output.\n",
        "def get_solidarity_label(text, model, tokenizer):\n",
        "    # Call your function that generates the JSON response\n",
        "    output = code_solidarity_label(text, model, tokenizer)\n",
        "    try:\n",
        "        # Parse the output (expected to be a JSON array with one object)\n",
        "        data = json.loads(output)\n",
        "        if isinstance(data, list) and len(data) > 0:\n",
        "            return data[0]\n",
        "        else:\n",
        "            return {\"Solidarity\": None, \"Justification\": None}\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"JSON decode error for text:\", text)\n",
        "        return {\"Solidarity\": None, \"Justification\": None}\n",
        "\n",
        "# Suppose your DataFrame 'df' has a column 'context' with the political statements.\n",
        "subset = df.sample(10000, random_state=99).reset_index(drop=True)\n",
        "\n",
        "# Apply the function to each row to obtain the label and justification as a dictionary.\n",
        "labels = subset['context'].apply(lambda x: get_solidarity_label(x, model, tokenizer))\n",
        "\n",
        "# Convert the resulting Series of dictionaries into a DataFrame.\n",
        "labels_df = pd.DataFrame(labels.tolist())\n",
        "\n",
        "# Concatenate the new columns to the original DataFrame.\n",
        "subset = pd.concat([subset, labels_df], axis=1)\n",
        "\n",
        "# Optionally, save the DataFrame to CSV (e.g., in Google Colab)\n",
        "subset.to_csv(\"df_results_with_labels.csv\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"df_results_with_labels.csv\")\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(subset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}