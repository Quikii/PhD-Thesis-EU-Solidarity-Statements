{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Data and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.1 Preprocessed Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2 Libraries (Especially Unsloth Download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model_Llama3.2_3B\", # Pretrained unsloth model \n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test Whether it Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"You are an expert researcher in political science. \"\n",
    "    \"Analyze the following text and determine which of the following solidarity categories best describe it: \"\n",
    "    \"Sentence to be analyzed: We as a Community of European States must hold together in times of crisis\"\n",
    "    \"POSSIBLE LABELS:\"\n",
    "    \"1. National Solidarity\"\n",
    "    \"2. European-Level Solidarity,\"\n",
    "    \"3. Outside-Europe Solidarity. \"\n",
    "    \"Decide which label is most suitable and provide the output in a JSON format like this:\"\n",
    "    \"{\\\"label\\\": \\\"<Insert one label: National Solidarity, European-Level Solidarity, or Outside-Europe Solidarity>\\\"}\"\n",
    "    \"ONLY WRITE IN THE JSON FORMAT. GENERATE NOTHING ELSE.\"\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"We as a Community of European States must hold together in times of crisis\"}\n",
    "]\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 300,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Extract Solidarity Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_solidarity_label(input_text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function instructs the model, an expert political scientist, to analyze a political statement \n",
    "    and decide if it expresses any indication of solidarity towards another actor.\n",
    "    The output is a JSON array with a single object containing two keys:\n",
    "      - \"Solidarity\": \"Solidarity\" if the statement shows explicit support, assistance, or positive alignment towards another party;\n",
    "                        \"No Solidarity\" otherwise.\n",
    "      - \"Justification\": A brief explanation (one or two sentences) justifying the label, referring to specific phrases if applicable.\n",
    "    \n",
    "    Parameters:\n",
    "        input_text (str): The text to analyze.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's generated response as a structured JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    instruction = (\n",
    "        \"You are an expert political scientist specializing in discourse analysis. \"\n",
    "        \"Your task is to analyze the following statement and determine whether it expresses any indication of solidarity towards another actor. \"\n",
    "        \"For your response, return a JSON array with a single object containing two keys: 'Solidarity' and 'Justification'. \"\n",
    "        \"The 'Solidarity' key should have the value 'Solidarity' if the statement shows support, or 'No Solidarity' if it does not. \"\n",
    "        \"The 'Justification' key should provide a brief explanation (one or two sentences) citing specific phrases or elements that justify your label. \\n\\n\"\n",
    "        \"For example, if the input is:\\n\"\n",
    "        \"\\\"I stand with you in these trying times, offering all my support,\\\" \\n\"\n",
    "        \"then the output should be:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"  {\\n\"\n",
    "        \"    \\\"Solidarity\\\": \\\"Solidarity\\\",\\n\"\n",
    "        \"    \\\"Justification\\\": \\\"The statement explicitly offers support and solidarity through phrases like 'I stand with you' and 'offering all my support'.\\\"\\n\"\n",
    "        \"  }\\n\"\n",
    "        \"]\\n\\n\"\n",
    "        \"If the input is:\\n\"\n",
    "        \"\\\"I am indifferent to your situation,\\\" \\n\"\n",
    "        \"then the output should be:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"  {\\n\"\n",
    "        \"    \\\"Solidarity\\\": \\\"No Solidarity\\\",\\n\"\n",
    "        \"    \\\"Justification\\\": \\\"The statement does not express any form of support or solidarity, as indicated by the word 'indifferent'.\\\"\\n\"\n",
    "        \"  }\\n\"\n",
    "        \"]\\n\\n\"\n",
    "        \"Now, analyze the following text:\\n\"\n",
    "        f\"{input_text}\"\n",
    "    )\n",
    "    \n",
    "    # Build the prompt (here we're combining the instruction and the input text)\n",
    "    prompt = instruction\n",
    "    \n",
    "    # Create the text-generation pipeline with a controlled temperature setting\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
    "    output = pipe(prompt, max_length=400)\n",
    "    generated_text = output[0]['generated_text'].strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Extract Solidarity Speech Acts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_solidarity_speech_acts(input_text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Function that instructs the model to analyze political text and extract solidarity-related speech acts,\n",
    "    returning a structured JSON array with the identified information.\n",
    "\n",
    "    Parameters:\n",
    "        input_text (str): The political text to be analyzed.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The coded response from the model as a structured JSON array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optimized instruction for extracting solidarity-related speech acts with structured output\n",
    "    instruction = (\n",
    "        \"You are an expert in political science and computational discourse analysis. \"\n",
    "        \"Your task is to analyze the following political text to identify and code all instances of solidarity-related speech acts. \"\n",
    "        \"A valid solidarity-related speech act must meet these criteria: \"\n",
    "        \"1. Transitive Structure: The utterance must contain a transitive verb structure linking a subject (the provider, typically the speaker) to a direct object (the recipient). \"\n",
    "        \"2. Positive Orientation: The subject expresses a positive orientation (they offer some form of assistance or help, even if verbal) toward the recipient, who is understood to be in a comparatively difficult situation. \"\n",
    "        \"3. Illocutionary Type: The speech act must clearly fall into one of the following categories: \"\n",
    "        \"   - Solidarity Expressives: Utterances that communicate emotional support or compassion without calling for specific action. \"\n",
    "        \"   - Solidarity Directives: Utterances that demand, request, or call on others to act in support of the recipient. \"\n",
    "        \"   - Solidarity Commissives: Utterances in which the speaker commits themselves to take future actions to support the recipient. \"\n",
    "        \"For each solidarity-related speech act identified in the text, extract and return the following information: \"\n",
    "        \"   - SpeechActType: Label the act as 'Expressive,' 'Directive,' or 'Commissive.' \"\n",
    "        \"   - Provider: Identify the actor (usually the speaker) performing the solidarity action. \"\n",
    "        \"   - Recipient: Identify the actor (direct object) who is in need and toward whom the positive orientation is directed. \"\n",
    "        \"   - PropositionalContent: Summarize the core message or demand embedded in the speech act. \"\n",
    "        \"If a sentence does not contain a solidarity-related speech act or lacks the required transitive structure, do not return anything. \"\n",
    "        \"Return your analysis as a structured JSON array where each element represents an identified speech act with the keys \"\n",
    "        \"'SpeechActType,' 'Provider,' 'Recipient,' and 'PropositionalContent.' \"\n",
    "        \"Do not include any additional commentary or text beyond this structured output. \"\n",
    "        \"For example, your output should look like this:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"  {\\n\"\n",
    "        \"    \\\"SpeechActType\\\": \\\"\\\",\\n\"\n",
    "        \"    \\\"Provider\\\": \\\"\\\",\\n\"\n",
    "        \"    \\\"Recipient\\\": \\\"\\\",\\n\"\n",
    "        \"    \\\"PropositionalContent\\\": \\\"\\\"\\n\"\n",
    "        \"  }\\n\"\n",
    "        \"]\\n\"\n",
    "        \"Text to analyze:\\n\"\n",
    "        f\"{input_text}\"\n",
    "    )\n",
    "\n",
    "    # Structuring the message with system and user roles\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    # Creating the text-generation pipeline with a deterministic setting\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
    "    output = pipe(messages)\n",
    "\n",
    "    # Return the generated response containing the structured JSON analysis\n",
    "    return output[0]['generated_text']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
