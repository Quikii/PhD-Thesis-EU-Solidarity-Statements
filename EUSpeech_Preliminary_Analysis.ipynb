{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quikii/PhD-Thesis-EU-Solidarity-Statements/blob/main/EUSpeech_Preliminary_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Import Data"
      ],
      "metadata": {
        "id": "DFSXYXQ4KrxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.1 EUSpeech Sample"
      ],
      "metadata": {
        "id": "agFLmNsIcMZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elRZkKsqvZkN"
      },
      "outputs": [],
      "source": [
        "from pandas import read_csv\n",
        "df = read_csv(\"/content/EUSpeech_translated.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.2 IOParlspeech"
      ],
      "metadata": {
        "id": "JM0BGtWXcO-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyreadr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWe38mBUcVO5",
        "outputId": "cc5ab8bf-7bf5-423f-a60f-2573bcd02468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyreadr\n",
            "  Downloading pyreadr-0.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyreadr) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->pyreadr) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->pyreadr) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->pyreadr) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadr) (1.17.0)\n",
            "Downloading pyreadr-0.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (411 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m389.1/411.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.7/411.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyreadr\n",
            "Successfully installed pyreadr-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyreadr\n",
        "\n",
        "# Create a list of country files\n",
        "files = [\n",
        "    \"/content/ioparlspeech.aus.rds\",\n",
        "    \"/content/ioparlspeech.can.rds\",\n",
        "    \"/content/ioparlspeech.deu.rds\",\n",
        "    \"/content/ioparlspeech.gbr.rds\",\n",
        "    \"/content/ioparlspeech.nzl.rds\",\n",
        "]\n",
        "\n",
        "# Initialize list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Load each file\n",
        "for file in files:\n",
        "    print(f\"Loading {file.split('/')[-1]}...\")\n",
        "    result = pyreadr.read_r(file)\n",
        "    temp_df = result[None]\n",
        "    # Add country identifier from filename\n",
        "    country = file.split('ioparlspeech.')[-1].split('.')[0].upper()\n",
        "    temp_df['country'] = country\n",
        "    dfs.append(temp_df)\n",
        "    print(f\"Successfully loaded {country}\")\n",
        "\n",
        "# Combine all dataframes\n",
        "import pandas as pd\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(\"\\nAll data loaded and combined!\")\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"Total number of rows: {len(df):,}\")\n",
        "print(\"\\nRows per country:\")\n",
        "print(df['country'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGL1RXk2cGNd",
        "outputId": "e01abf44-972d-4cc2-dcac-676493f1703c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ioparlspeech.aus.rds...\n",
            "Successfully loaded AUS\n",
            "Loading ioparlspeech.can.rds...\n",
            "Successfully loaded CAN\n",
            "Loading ioparlspeech.deu.rds...\n",
            "Successfully loaded DEU\n",
            "Loading ioparlspeech.gbr.rds...\n",
            "Successfully loaded GBR\n",
            "Loading ioparlspeech.nzl.rds...\n",
            "Successfully loaded NZL\n",
            "\n",
            "All data loaded and combined!\n",
            "\n",
            "Dataset Overview:\n",
            "Total number of rows: 527,141\n",
            "\n",
            "Rows per country:\n",
            "country\n",
            "GBR    280602\n",
            "CAN    101371\n",
            "DEU     76876\n",
            "AUS     42450\n",
            "NZL     25842\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.1 Download Libraries"
      ],
      "metadata": {
        "id": "CXBJGDabKzU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.0 Import Libraries"
      ],
      "metadata": {
        "id": "7wD9UQLeLGJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "#import coreferee\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm  # For progress display"
      ],
      "metadata": {
        "id": "5er7ImXqLF4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Load Spacy Models\n"
      ],
      "metadata": {
        "id": "1gItayMMMkbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.1 Tokenize into Sentences"
      ],
      "metadata": {
        "id": "NhSf21_EVtJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Ensure required NLTK resources are available\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt_tab/english\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt_tab\")\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Function to get rokenized words and sentences\n",
        "# ----------------------------\n",
        "def tokenize_text(df, text_col=\"text.en\", pattern_str=r\"\\bsolidar\\w*\\b\"):\n",
        "    \"\"\"\n",
        "    Filters rows containing a specified pattern in the text, tokenizes the text into words,\n",
        "    and splits the text into sentences.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input DataFrame with a text column.\n",
        "        text_col (str): Name of the column containing text (default \"text.en\").\n",
        "        pattern_str (str): Regex pattern string to filter rows (default searches for words starting with \"solidar\").\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with two new columns:\n",
        "            - 'tokenized': List of tokens from the text.\n",
        "            - 'sentences': List of sentences from the text.\n",
        "    \"\"\"\n",
        "    # Compile the regex pattern\n",
        "    pattern = re.compile(pattern_str, re.IGNORECASE)\n",
        "\n",
        "    # Filter rows that match the pattern in the specified text column\n",
        "    df_filtered = df[df[text_col].apply(lambda x: bool(pattern.search(x)))].copy()\n",
        "\n",
        "    # Tokenize the text into words and store in a new column 'tokenized'\n",
        "    df_filtered['tokenized'] = df_filtered[text_col].apply(word_tokenize)\n",
        "\n",
        "    # Split the text into sentences and store in a new column 'sentences'\n",
        "    df_filtered['sentences'] = df_filtered[text_col].apply(sent_tokenize)\n",
        "\n",
        "    return df_filtered\n"
      ],
      "metadata": {
        "id": "1CILJIrUVv_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e973b1-bca6-4a50-f05f-b574984c36a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Resolve Coreference Function"
      ],
      "metadata": {
        "id": "66Gu_khjK8yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.0/en_coreference_web_trf-3.4.0a0-py3-none-any.whl#egg=en_coreference_web_trf\n"
      ],
      "metadata": {
        "id": "xpaCynsSKHqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!pip show spacy\n",
        "import spacy_experimental\n",
        "nlp = spacy.load(\"en_coreference_web_trf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chInPPacoqNa",
        "outputId": "fda73401-27c7-4782-bf9d-82f53d21a543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spacy\n",
            "Version: 3.4.4\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, pathy, preshed, pydantic, requests, setuptools, smart-open, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi\n",
            "Required-by: en-coreference-web-trf, fastai, spacy-experimental, spacy-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def append_corefs(doc):\n",
        "    \"\"\"\n",
        "    Returns a version of the document text where for each non-main coreferent mention,\n",
        "    the main mention is appended in square brackets.\n",
        "\n",
        "    For example, if a mention is \"he\" and the main mention for that coreference chain is \"Fidel Castro\",\n",
        "    the function will change \"he\" to \"he [Fidel Castro]\".\n",
        "\n",
        "    Args:\n",
        "        doc (spacy.tokens.Doc): A spaCy document processed with a coreference model that sets doc.spans.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified text with coreference annotations appended.\n",
        "    \"\"\"\n",
        "    resolved_text = doc.text\n",
        "    offset = 0\n",
        "    insertions = []\n",
        "\n",
        "    # Iterate over each coreference chain in doc.spans.\n",
        "    for chain in doc.spans:\n",
        "        # Process only chains that begin with the expected prefix.\n",
        "        if not chain.startswith(\"coref_clusters\"):\n",
        "            continue\n",
        "\n",
        "        # The first mention is the main reference.\n",
        "        main_text = doc.spans[chain][0].text\n",
        "\n",
        "        # For every subsequent mention, we want to append the main text in brackets.\n",
        "        for idx, span in enumerate(doc.spans[chain]):\n",
        "            if idx > 0:\n",
        "                # Insert after the mention; use span.end_char as the insertion point.\n",
        "                insertions.append([span.end_char, f\" [{main_text}]\"])\n",
        "\n",
        "    # Sort the insertions by the character index.\n",
        "    for pos, insertion in sorted(insertions, key=lambda x: x[0]):\n",
        "        resolved_text = resolved_text[:pos + offset] + insertion + resolved_text[pos + offset:]\n",
        "        offset += len(insertion)\n",
        "\n",
        "    return resolved_text\n",
        "\n",
        "# Example usage:\n",
        "import spacy\n",
        "import spacy_experimental\n",
        "\n",
        "nlp = spacy.load(\"en_coreference_web_trf\")\n",
        "text = \"\"\"Fidel Castro led a communist revolution that toppled the Cuban government in 1959, after which he declared himself prime minister. He held the title until 1976, when it was abolished and he became head of the Communist Party and president of the council of state and the council of ministers. With his health failing, Castro handed power to his brother, Raúl, in 2006. He died in 2016.\"\"\"\n",
        "doc = nlp(text)\n",
        "resolved = append_corefs(doc)\n",
        "print(\"Original text:\")\n",
        "print(doc.text)\n",
        "print(\"\\nText with coreferences appended:\")\n",
        "print(resolved)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ7m75mOxIxt",
        "outputId": "94a29d6f-de16-400c-8c97-82a8079dc0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "Fidel Castro led a communist revolution that toppled the Cuban government in 1959, after which he declared himself prime minister. He held the title until 1976, when it was abolished and he became head of the Communist Party and president of the council of state and the council of ministers. With his health failing, Castro handed power to his brother, Raúl, in 2006. He died in 2016.\n",
            "\n",
            "Text with coreferences appended:\n",
            "Fidel Castro led a communist revolution that toppled the Cuban government in 1959, after which he declared [Fidel Castro led] himself prime [Fidel Castro led] minister. He held [Fidel Castro led] the title until [himself prime minister.] 1976, when it was [himself prime minister.] abolished and he became [Fidel Castro led] head of the Communist Party and president of the council of state and the council of ministers. With his health [Fidel Castro led] failing, Castro handed [Fidel Castro led] power to his brother [Fidel Castro led], Raúl, in 2006. He died [Fidel Castro led] in 2016.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Define Processing Function"
      ],
      "metadata": {
        "id": "8kF8NuM6MpQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTXfHh1cvsGl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Define processing function for each text entry\n",
        "# ----------------------------\n",
        "\n",
        "# ----------------------------\n",
        "# Define processing function for each text entry\n",
        "# ----------------------------\n",
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Process a single text:\n",
        "      - Extract Named Entities, subject, and object from the original text.\n",
        "      - Resolve coreference using coreferee (via append_corefs) on the entire text.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sentence_records, resolved_text)\n",
        "\n",
        "        - sentence_records: A list of sentence-level dictionaries with keys:\n",
        "            \"sentence\", \"entities\", \"subject\", and \"object\".\n",
        "        - resolved_text: The entire text with coreference annotations appended.\n",
        "    \"\"\"\n",
        "    # Process the original text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    sentence_records = []\n",
        "    # Extract sentence-level data from the original text\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        if not sent_text:\n",
        "            continue\n",
        "\n",
        "        sent_doc = nlp(sent_text)\n",
        "        entities = [(ent.text, ent.label_) for ent in sent_doc.ents]\n",
        "\n",
        "        subject = None\n",
        "        obj = None\n",
        "        for token in sent_doc:\n",
        "            if token.dep_ in (\"nsubj\", \"nsubjpass\") and subject is None:\n",
        "                subject = token.text\n",
        "            if token.dep_ in (\"dobj\", \"pobj\") and obj is None:\n",
        "                obj = token.text\n",
        "\n",
        "        sentence_records.append({\n",
        "            \"sentence\": sent_text,\n",
        "            \"entities\": entities,\n",
        "            \"subject\": subject,\n",
        "            \"object\": obj\n",
        "        })\n",
        "\n",
        "    # Apply coreference resolution using append_corefs\n",
        "    resolved_text = append_corefs(doc)\n",
        "\n",
        "    return sentence_records, resolved_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 Run the Pre-Processing"
      ],
      "metadata": {
        "id": "bTsaD6fyMui5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# ----------------------------\n",
        "# Filter DataFrame rows by \"solidarity\" (or related forms)\n",
        "# ----------------------------\n",
        "\n",
        "pattern = re.compile(r\"\\bsolidar\\w*\\b\", re.IGNORECASE)\n",
        "filtered_df = df[df[\"text.en\"].apply(lambda x: bool(pattern.search(x)))].copy()\n",
        "\n",
        "# ----------------------------\n",
        "# Parallel processing over the filtered texts with progress display\n",
        "# ----------------------------\n",
        "\n",
        "results_sentences = []\n",
        "results_resolved = []\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    future_to_text = {executor.submit(process_text, text): text for text in filtered_df[\"text.en\"]}\n",
        "    for future in tqdm(concurrent.futures.as_completed(future_to_text),\n",
        "                       total=len(future_to_text),\n",
        "                       desc=\"Processing texts\"):\n",
        "        try:\n",
        "            sentence_records, resolved_text = future.result()\n",
        "            results_sentences.extend(sentence_records)\n",
        "            results_resolved.append(resolved_text)\n",
        "        except Exception as exc:\n",
        "            print(f\"Error processing a text: {exc}\")\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Create a DataFrame of sentence-level records\n",
        "# ----------------------------\n",
        "\n",
        "df_sentences = pd.DataFrame(results_sentences)\n",
        "print(df_sentences.head())"
      ],
      "metadata": {
        "id": "Wtmf22ZYMwvt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a82d2781-3621-429c-e97e-1800b0fa7093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing texts:   0%|          | 0/2256 [00:42<?, ?it/s]\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-30-7095f0cc4be9>\", line 17, in <cell line: 0>\n",
            "    for future in tqdm(concurrent.futures.as_completed(future_to_text),\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 243, in as_completed\n",
            "    waiter.event.wait(wait_timeout)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 629, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 327, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-30-7095f0cc4be9>\", line 15, in <cell line: 0>\n",
            "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 647, in __exit__\n",
            "    self.shutdown(wait=True)\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 235, in shutdown\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7095f0cc4be9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfuture_to_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text.en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     for future in tqdm(concurrent.futures.as_completed(future_to_text),\n\u001b[0m\u001b[1;32m     18\u001b[0m                        \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_to_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-7095f0cc4be9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mresults_resolved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfuture_to_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text.en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Running LLM"
      ],
      "metadata": {
        "id": "3X1CoWKoMx8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 2.0 Download Libraries"
      ],
      "metadata": {
        "id": "WHRAdj5Q79R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "-oPFj7QINNw8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTLUbaH39PWA",
        "outputId": "dc87a817-8f0d-4f3f-b18a-fd0864ea409d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from xformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->xformers) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m857.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Initialize Unsloth"
      ],
      "metadata": {
        "id": "jFSW8PQUOIs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1.1 Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "nGvoOSyYOOFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2000 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "20f500e38ccc4adb97613a447951789d",
            "fcf8aa9fb26148febf96cb467592297e",
            "ff42af112f294ca1bc104454fc351f69",
            "3d01ac28d3ba43409cccc869b682c6ee",
            "dc74d12374b641caad89402e9d26877c",
            "8e6a0617b99a455789ea921268412e3a",
            "c0f6ef6555d14164a5e4af9f6f62ae3d",
            "25030c2897624d4490922cda5351430b",
            "1120dc371a2b427fb6584510da134035",
            "ea4a6781d9d44303a9bec089c0eed203",
            "e122af4ae199455ab3d6053ddf5a3803",
            "3bcde2fb21064fe6a5453775af590933",
            "8e64e947b85b4aab9700308ebc0231f1",
            "b5a8008648054c3c9c6da8ee9b4a74b3",
            "67c7d7acc2754e8f81e2fcf90f55c4bb",
            "af4e1ded3906453888c235da83191166",
            "2478558c01b04188aafcdb52a4a573de",
            "9eebd25e88974fe4bbf4b1d6083334a8",
            "72387e3bd8624018be1e6fef764ba99f",
            "5ec71d9a13514d639d94638acb4155e6",
            "3baae1d414bd47a6a8f994ef9d5c269d",
            "f8fc6461f40b42fd87d2a3358442a07e"
          ]
        },
        "id": "fwQl-5gLNK1H",
        "outputId": "4752b215-93fa-47db-b14a-31167eead723"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20f500e38ccc4adb97613a447951789d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bcde2fb21064fe6a5453775af590933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.18 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1.2 Set-up the Peft Model"
      ],
      "metadata": {
        "id": "G9lZVR2gOS4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "QH_s5bGMNVEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c69065-e085-4ceb-ea2e-db5da6c3786d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1.3 Test Whether it Works"
      ],
      "metadata": {
        "id": "fAERABNKOaWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = (\n",
        "    \"You are an expert researcher in political science. \"\n",
        "    \"Analyze the following text and determine which of the following solidarity categories best describe it: \"\n",
        "    \"Sentence to be analyzed: We as a Community of European States must hold together in times of crisis\"\n",
        "    \"POSSIBLE LABELS:\"\n",
        "    \"1. National Solidarity\"\n",
        "    \"2. European-Level Solidarity,\"\n",
        "    \"3. Outside-Europe Solidarity. \"\n",
        "    \"Decide which label is most suitable and provide the output in a JSON format like this:\"\n",
        "    \"{\\\"label\\\": \\\"<Insert one label: National Solidarity, European-Level Solidarity, or Outside-Europe Solidarity>\\\"}\"\n",
        "    \"ONLY WRITE IN THE JSON FORMAT. GENERATE NOTHING ELSE.\"\n",
        ")\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": instruction},\n",
        "    {\"role\": \"user\", \"content\": \"We as a Community of European States must hold together in times of crisis\"}\n",
        "]\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 300,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "id": "TXge9WaLNVua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8d7444-c5bb-4109-832d-445a0c552c5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"label\": \"European-Level Solidarity\"}<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Prompt for Free-Coding"
      ],
      "metadata": {
        "id": "04HA8jyrQTml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 Write out the prompt"
      ],
      "metadata": {
        "id": "RiOKYkYmQ9cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def code_text_with_theme(input_text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Function that instructs the model to categorize the given text into solidarity categories.\n",
        "\n",
        "    Parameters:\n",
        "        input_text (str): The text to be analyzed.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The coded response from the model containing one or more solidarity category labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Optimized instruction for categorizing text into solidarity categories\n",
        "    instruction = (\n",
        "        \"You are an expert researcher in political science. \"\n",
        "        \"Analyze the following text and determine which of the following solidarity categories best describe it: \"\n",
        "        \"National Solidarity, European-Level Solidarity, Outside-Europe Solidarity. \"\n",
        "        \"If the text aligns with more than one category, return a comma-separated list of the applicable categories. \"\n",
        "        \"Return only the category labels with no additional commentary.\"\n",
        "        \"Write Neutral, if the sentence cannot be linked to solidarity.\"\n",
        "    )\n",
        "\n",
        "    # Structuring the message with system and user roles\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "\n",
        "    # Creating the text-generation pipeline with a deterministic setting\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
        "    output = pipe(messages)\n",
        "\n",
        "    # Return the generated response containing the solidarity categories\n",
        "    return output[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "HLol0pQQQV7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Prompt for Specific Codes"
      ],
      "metadata": {
        "id": "-gDG5_drQW5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def code_solidarity_categories(input_text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Function that instructs the model to categorize the given text into solidarity categories.\n",
        "\n",
        "    Parameters:\n",
        "        input_text (str): The text to be analyzed.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The coded response from the model containing one or more solidarity category labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Optimized instruction for categorizing text into solidarity categories\n",
        "    instruction = (\n",
        "        \"You are an expert researcher in political science. \"\n",
        "        \"Analyze the following text and determine which of the following solidarity categories best describe it: \"\n",
        "        \"National Solidarity, European-Level Solidarity, Outside-Europe Solidarity. \"\n",
        "        \"If the text aligns with more than one category, return a comma-separated list of the applicable categories. \"\n",
        "        \"Return only the category labels with no additional commentary.\"\n",
        "        \"Write Neutral, if the sentence cannot be linked to solidarity.\"\n",
        "    )\n",
        "\n",
        "    # Structuring the message with system and user roles\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "\n",
        "    # Creating the text-generation pipeline with a deterministic setting\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
        "    output = pipe(messages)\n",
        "\n",
        "    # Return the generated response containing the solidarity categories\n",
        "    return output[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "0kIUD7mGQZRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 Prompt for Solidarity Speech Acts"
      ],
      "metadata": {
        "id": "rWcn9q6ebpg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def code_solidarity_speech_acts(input_text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Function that instructs the model to analyze political text and extract solidarity-related speech acts,\n",
        "    returning a structured JSON array with the identified information.\n",
        "\n",
        "    Parameters:\n",
        "        input_text (str): The political text to be analyzed.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The coded response from the model as a structured JSON array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Optimized instruction for extracting solidarity-related speech acts with structured output\n",
        "    instruction = (\n",
        "        \"You are an expert in political science and computational discourse analysis. \"\n",
        "        \"Your task is to analyze the following political text to identify and code all instances of solidarity-related speech acts. \"\n",
        "        \"A valid solidarity-related speech act must meet these criteria: \"\n",
        "        \"1. Transitive Structure: The utterance must contain a transitive verb structure linking a subject (the provider, typically the speaker) to a direct object (the recipient). \"\n",
        "        \"2. Positive Orientation: The subject expresses a positive orientation (they offer some form of assistance or help, even if verbal) toward the recipient, who is understood to be in a comparatively difficult situation. \"\n",
        "        \"3. Illocutionary Type: The speech act must clearly fall into one of the following categories: \"\n",
        "        \"   - Solidarity Expressives: Utterances that communicate emotional support or compassion without calling for specific action. \"\n",
        "        \"   - Solidarity Directives: Utterances that demand, request, or call on others to act in support of the recipient. \"\n",
        "        \"   - Solidarity Commissives: Utterances in which the speaker commits themselves to take future actions to support the recipient. \"\n",
        "        \"For each solidarity-related speech act identified in the text, extract and return the following information: \"\n",
        "        \"   - SpeechActType: Label the act as 'Expressive,' 'Directive,' or 'Commissive.' \"\n",
        "        \"   - Provider: Identify the actor (usually the speaker) performing the solidarity action. \"\n",
        "        \"   - Recipient: Identify the actor (direct object) who is in need and toward whom the positive orientation is directed. \"\n",
        "        \"   - PropositionalContent: Summarize the core message or demand embedded in the speech act. \"\n",
        "        \"If a sentence does not contain a solidarity-related speech act or lacks the required transitive structure, do not return anything. \"\n",
        "        \"Return your analysis as a structured JSON array where each element represents an identified speech act with the keys \"\n",
        "        \"'SpeechActType,' 'Provider,' 'Recipient,' and 'PropositionalContent.' \"\n",
        "        \"Do not include any additional commentary or text beyond this structured output. \"\n",
        "        \"For example, your output should look like this:\\n\"\n",
        "        \"[\\n\"\n",
        "        \"  {\\n\"\n",
        "        \"    \\\"SpeechActType\\\": \\\"\\\",\\n\"\n",
        "        \"    \\\"Provider\\\": \\\"\\\",\\n\"\n",
        "        \"    \\\"Recipient\\\": \\\"\\\",\\n\"\n",
        "        \"    \\\"PropositionalContent\\\": \\\"\\\"\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"]\\n\"\n",
        "        \"Text to analyze:\\n\"\n",
        "        f\"{input_text}\"\n",
        "    )\n",
        "\n",
        "    # Structuring the message with system and user roles\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": instruction}\n",
        "    ]\n",
        "\n",
        "    # Creating the text-generation pipeline with a deterministic setting\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.9)\n",
        "    output = pipe(messages)\n",
        "\n",
        "    # Return the generated response containing the structured JSON analysis\n",
        "    return output[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "RhLN-Qc2cBKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Prompt Testing"
      ],
      "metadata": {
        "id": "xuWwoavrWhwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.1 Levels of Solidarity run LLM"
      ],
      "metadata": {
        "id": "sLhHGFp-c-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "subset = df_sentences['sentence'].sample(5, random_state=42).reset_index(drop=True)\n",
        "# Initialize an empty list for existing labels (for theme extraction)\n",
        "existing_labels = []\n",
        "\n",
        "# Process the subset of 20 sentences and store results in a list\n",
        "solidarity_collect = []\n",
        "for sentence in subset:\n",
        "    solidarity = code_text_with_theme(sentence,  model = model, tokenizer = tokenizer)\n",
        "    solidarity_collect.append(solidarity)\n",
        "\n",
        "\n",
        "# Create a results dataframe and display it\n",
        "#results_df = pd.DataFrame(solidarity)\n",
        "print(solidarity[2][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "DSTcdBkSWjs7",
        "outputId": "ba844f98-8487-4965-f447-5dda9cc2fbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ff339c2d6d41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Create a results dataframe and display it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolidarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solidarity_collect[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxEF1QPnS_ir",
        "outputId": "2bd9c46a-d2c4-4bbc-e126-abc777f7fe65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are an expert researcher in political science. Analyze the following text and determine which of the following solidarity categories best describe it: National Solidarity, European-Level Solidarity, Outside-Europe Solidarity. If the text aligns with more than one category, return a comma-separated list of the applicable categories. Return only the category labels with no additional commentary.Write Neutral, if the sentence cannot be linked to solidarity.'}, {'role': 'user', 'content': 'The Council also welcomes the establishment of a study group to identify, in the short term, concrete measures to make more efficient use of existing policies and instruments, in particular in the area of \\u200b\\u200bcooperation with countries of origin and transit, the activities of the European Agency for the Management of Operational Cooperation at the External Borders of the Member States of the Union (FRONTEX) and the fight against human trafficking and illegal entry of migrants.'}, {'role': 'assistant', 'content': 'It seems you\\'ve provided a text that is intentionally similar in structure and content to the previous text. Let\\'s analyze this text:\\n\\n* The text starts with \"The Council also welcomes...\" which is a similar sentence structure to the previous text.\\n* The phrase \"establishment of a study group\" is repeated, which suggests a sense of continuity or repetition.\\n* The use of words like \"concrete\", \"short\", \"term\", and \"measures\" indicates a focus on practical, action-oriented steps.\\n* The phrase \"in particular in the area of cooperation\" suggests a focus on collaboration and coordination with other countries or organizations.\\n* The mention of \"FRONTEX\" and the fight against \"human trafficking and illegal entry of migrants\" introduces a specific context or issue, which could be related to immigration or border control policies.\\n\\nBased on this analysis, I would categorize the text as:\\n\\nNational Solidarity: The text mentions a Council, which is a national-level organization, and a focus on cooperation and coordination with other countries.\\n\\nEuropean-Level Solidarity: The text also mentions European institutions, such as FRONTEX, and a focus on cooperation and coordination with other European countries.\\n\\nOutside-Europe Solidarity: The text does not mention any outside European solidarity, and the focus is primarily on national and European cooperation.\\n\\nSo, the three solidarity categories applicable to this text are:\\n\\n1. National Solidarity\\n2. European-Level Solidarity\\n3. Outside-Europe Solidarity is not applicable, but National Solidarity and European-Level Solidarity are.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.2 Levels of Solidarity Disaggregate Output"
      ],
      "metadata": {
        "id": "SY1Sq_93dHV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = []\n",
        "for conv in solidarity_collect:\n",
        "    for message in conv:\n",
        "        if message.get(\"role\") == \"assistant\":\n",
        "            responses.append(message.get(\"content\").strip())\n",
        "print(responses)\n",
        "sentence = []\n",
        "for conv in solidarity_collect:\n",
        "    for message in conv:\n",
        "        if message.get(\"role\") == \"user\":\n",
        "            sentence.append(message.get(\"content\").strip())\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcFdgZD9YlqD",
        "outputId": "4c1d5d4c-b6c7-4e5a-e5d0-0d500db92c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Neutral', 'European-Level Solidarity, National Solidarity', 'Outside-Europe Solidarity', 'Neutral', 'Neutral']\n",
            "['Thank you very much for this visit, Paolo, and welcome to Spain.', 'More recently we have also made important contributions to the design of the Banking Union and to new integration objectives, such as the Common Energy Market, the digital economy, services or the Economic, Fiscal and Political Union projects.', 'The Council also welcomes the establishment of a study group to identify, in the short term, concrete measures to make more efficient use of existing policies and instruments, in particular in the area of \\u200b\\u200bcooperation with countries of origin and transit, the activities of the European Agency for the Management of Operational Cooperation at the External Borders of the Member States of the Union (FRONTEX) and the fight against human trafficking and illegal entry of migrants.', 'And I propose that we proclaim together the nouns progress, development, coexistence, cleanliness.', 'That is a contribution to future generations.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.1 Solidarity Speech Acts"
      ],
      "metadata": {
        "id": "tAeRDqmYdPui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "sentences = [\n",
        "    \"In times of crisis, solidarity is our strongest weapon.\",\n",
        "    \"True democracy thrives when we stand together as one.\",\n",
        "    \"National unity begins with mutual respect and shared purpose.\",\n",
        "    \"Let us build a society rooted in compassion and collective strength.\",\n",
        "    \"Solidarity is not just a word — it's a commitment to one another.\",\n",
        "    \"We rise by lifting others, hand in hand, across all communities.\",\n",
        "    \"A just world requires that no one is left behind.\",\n",
        "    \"Only through unity can we overcome the challenges of our time.\",\n",
        "    \"Our strength lies not in division, but in shared struggle and hope.\",\n",
        "    \"Solidarity means fighting for equality, justice, and human dignity for all.\"\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_sentences = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "subset = df_sentences['sentence'].sample(1, random_state=99).reset_index(drop=True)\n",
        "# Initialize an empty list for existing labels (for theme extraction)\n",
        "existing_labels = []\n",
        "\n",
        "# Process the subset of 20 sentences and store results in a list\n",
        "solidarity_collect = []\n",
        "for sentence in subset:\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    solidarity = code_solidarity_speech_acts(sentence, model, tokenizer)\n",
        "    solidarity_collect.append(solidarity)\n",
        "\n",
        "\n",
        "# Create a results dataframe and display it\n",
        "#results_df = pd.DataFrame(results)\n",
        "print(solidarity[2][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "d_c4vSLCdSfS",
        "outputId": "6a41cb2e-4fdb-4a3b-fd93-8cb4ff0956cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4133d1b0817f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Create a results dataframe and display it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#results_df = pd.DataFrame(results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolidarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solidarity_collect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkAo65n4j3Vz",
        "outputId": "78c8c4b4-2f3e-4c05-de98-a226e559210f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'role': 'user', 'content': 'You are an expert in political science and computational discourse analysis. Your task is to analyze the following political text to identify and code all instances of solidarity-related speech acts. A valid solidarity-related speech act must meet these criteria: 1. Transitive Structure: The utterance must contain a transitive verb structure linking a subject (the provider, typically the speaker) to a direct object (the recipient). 2. Positive Orientation: The subject expresses a positive orientation (they offer some form of assistance or help, even if verbal) toward the recipient, who is understood to be in a comparatively difficult situation. 3. Illocutionary Type: The speech act must clearly fall into one of the following categories:    - Solidarity Expressives: Utterances that communicate emotional support or compassion without calling for specific action.    - Solidarity Directives: Utterances that demand, request, or call on others to act in support of the recipient.    - Solidarity Commissives: Utterances in which the speaker commits themselves to take future actions to support the recipient. For each solidarity-related speech act identified in the text, extract and return the following information:    - SpeechActType: Label the act as \\'Expressive,\\' \\'Directive,\\' or \\'Commissive.\\'    - Provider: Identify the actor (usually the speaker) performing the solidarity action.    - Recipient: Identify the actor (direct object) who is in need and toward whom the positive orientation is directed.    - PropositionalContent: Summarize the core message or demand embedded in the speech act. If a sentence does not contain a solidarity-related speech act or lacks the required transitive structure, do not return anything. Return your analysis as a structured JSON array where each element represents an identified speech act with the keys \\'SpeechActType,\\' \\'Provider,\\' \\'Recipient,\\' and \\'PropositionalContent.\\' Do not include any additional commentary or text beyond this structured output. For example, your output should look like this:\\n[\\n  {\\n    \"SpeechActType\": \"\",\\n    \"Provider\": \"\",\\n    \"Recipient\": \"\",\\n    \"PropositionalContent\": \"\"\\n  }\\n]\\nText to analyze:\\nOur strength lies not in division, but in shared struggle and hope.'}, {'role': 'assistant', 'content': \"It seems like we're having a fun conversation about the intricacies of language and communication. However, I'll make sure to get back on track and provide a helpful response.\\n\\nBased on our conversation, I'll provide a brief summary of the key points:\\n\\n1.  We discussed the importance of identifying and analyzing solidarity-related speech acts in political discourse.\\n2.  We went over the criteria for a valid solidarity-related speech act, including a transitive structure, positive orientation, and illocutionary type.\\n3.  We touched on the various types of solidarity-related speech acts, such as expressives, directives, and commissives.\\n4.  We mentioned the need to identify the provider, recipient, and propositional content in solidarity-related speech acts.\\n\\nIf you'd like to explore this topic further or discuss other ideas, I'm here to help!\\n\\nToday's Date: 26 July 2024\"}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "results = []\n",
        "for conv in solidarity_collect:\n",
        "    original_sentence = None\n",
        "    for message in conv:\n",
        "        # Extract the original sentence from the user message for reference\n",
        "        if message.get(\"role\") == \"user\":\n",
        "            original_sentence = message.get(\"content\").strip()\n",
        "        # Process the assistant's response which should contain JSON data\n",
        "        elif message.get(\"role\") == \"assistant\":\n",
        "            assistant_text = message.get(\"content\").strip()\n",
        "            try:\n",
        "                # Parse the JSON array output from the assistant\n",
        "                speech_acts = json.loads(assistant_text)\n",
        "                # For each identified speech act, add an entry to the results\n",
        "                for act in speech_acts:\n",
        "                    # Optionally include the original sentence for reference\n",
        "                    act[\"OriginalSentence\"] = original_sentence\n",
        "                    results.append(act)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Error: Could not decode JSON for assistant message:\\n\", assistant_text)\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Display the resulting DataFrame with separate columns for each part of the response\n",
        "print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpMOIbNjdWqD",
        "outputId": "683899c2-567f-4696-fb58-78159d0c8741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not decode JSON for assistant message:\n",
            " You're making a clever transition from the language of solidarity to the language of solidarity.\n",
            "\n",
            "To continue the analysis:\n",
            "\n",
            "It seems that the original text was trying to convey the idea that solidarity is about unity and mutual support, rather than division or separation. The correct interpretation of solidarity-related speech acts involves identifying the types of speech acts, such as expressive, directive, or commissive, and understanding the context in which they are used.\n",
            "\n",
            "Let's break down the key concepts:\n",
            "\n",
            "*   Speech act: A speech act is a single utterance or message that conveys a particular meaning or intent.\n",
            "*   Speech act type: Speech act types include expressive, directive, or commissive, which refer to the speaker's intent or the action's effect.\n",
            "*   Speech act context: The context in which a speech act is used can include the situation, the audience, and the speaker's relationship with the audience.\n",
            "\n",
            "By understanding these concepts, we can better analyze and interpret solidarity-related speech acts in the context of the original text.\n",
            "Error: Could not decode JSON for assistant message:\n",
            " You're quoting the famous phrase by Robert Schuman, but also referencing the original solidarity-related speech act criteria. Let's try to analyze the given text from a different perspective:\n",
            "\n",
            "The text \"We rise by lifting others, hand in hand, across all communities\" can be analyzed as a solidarity-related speech act with the following characteristics:\n",
            "\n",
            "1. **Transitive structure**: The sentence has a clear subject-verb-object structure, with \"We\" as the subject, \"rise\" as the verb, and \"by lifting others\" as the complement.\n",
            "2. **Positive orientation**: The text expresses a positive sentiment, with the phrase \"We rise\" implying uplift, progress, and empowerment.\n",
            "3. **Illocutionary type**: The sentence can be classified as a commissive, as it commits the speaker to taking action (lifting others) and implies a sense of shared responsibility.\n",
            "\n",
            "From a computational discourse analysis perspective, this text can be represented as a structured JSON object with the following properties:\n",
            "\n",
            "*   \"SpeechActType\": \"commissive\"\n",
            "*   \"Provider\": \"speaker\" ( implied by \"We\" )\n",
            "*   \"Recipient\": \"all communities\"\n",
            "*   \"PropositionalContent\": \"We rise by lifting others, hand in hand, across all communities\"\n",
            "\n",
            "Please note that this analysis is based on the provided text and the initial solidarity-related speech act criteria. The actual interpretation may vary depending on the context and the specific criteria used.\n",
            "Error: Could not decode JSON for assistant message:\n",
            " A poignant and profound statement! You're highlighting the importance of solidarity as a social force that brings people together, especially in times of need or struggle. It's a reminder that collective action and support can make a significant difference in people's lives.\n",
            "\n",
            "In the context of your original prompt, I'm assuming you're exploring the complexities of solidarity-related speech acts, which involve using language to build relationships, establish empathy, and promote mutual understanding.\n",
            "\n",
            "If you'd like to delve deeper into this topic, I'd be happy to discuss the following:\n",
            "\n",
            "1. The role of language in constructing solidarity: How do linguistic features, such as tone, pitch, and syntax, contribute to the expression of solidarity?\n",
            "2. The performative function of solidarity-related speech acts: How do these speech acts create social reality, shape relationships, and establish norms?\n",
            "3. The impact of solidarity-related speech acts on individuals and communities: How do these speech acts affect people's experiences, perceptions, and behaviors?\n",
            "\n",
            "Let me know if you'd like to explore any of these topics further!\n",
            "Error: Could not decode JSON for assistant message:\n",
            " A poignant point!\n",
            "\n",
            "Indeed, national unity is built on a foundation of mutual respect, shared purpose, and collective understanding. When individuals and communities respect each other's differences, values, and cultures, they are more likely to come together and find common ground.\n",
            "\n",
            "This is reflected in the concept of \"speech acts,\" where the way we communicate can either unite or divide us. By using language that is inclusive, empathetic, and respectful, we can create a social climate that fosters national unity.\n",
            "\n",
            "In this spirit, I'd like to propose a new speech act: the \"Unity Utterance.\" This could be a phrase or sentence that acknowledges our shared humanity, promotes mutual understanding, and encourages collective action.\n",
            "\n",
            "For example: \"We stand together as a nation, united in our diversity, and committed to building a brighter future for all.\"\n",
            "\n",
            "The Unity Utterance acknowledges our differences, but emphasizes our shared goals and values. By using this phrase, we can create a cultural narrative that promotes national unity, mutual respect, and shared purpose.\n",
            "\n",
            "What do you think?\n",
            "Error: Could not decode JSON for assistant message:\n",
            " A poignant conclusion to our analysis.\n",
            "\n",
            "Let's summarize our discussion:\n",
            "\n",
            "* We analyzed the concept of solidarity-related speech acts in a political science and computational discourse analysis context.\n",
            "* We identified the importance of a transitive structure, positive orientation, and illocutionary type in a solidarity-related speech act.\n",
            "* We discussed the characteristics of expressive, directive, and commissive speech acts in the context of solidarity.\n",
            "* We emphasized the importance of identifying the speaker, action, and audience in a solidarity-related speech act.\n",
            "* We returned to the theme of a just world, where no one is left behind, and solidarity is essential for achieving this goal.\n",
            "\n",
            "Thank you for engaging in this analysis with me. If you have any further questions or topics you'd like to discuss, please feel free to ask!\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrame to CSV file in the current Colab working directory\n",
        "df_results.to_csv(\"df_results.csv\", index=False)\n",
        "\n",
        "# If you want to download the file to your local machine, use:\n",
        "from google.colab import files\n",
        "files.download(\"df_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "t245uZ8_gxui",
        "outputId": "78eb8151-fc2d-4470-9412-6f78f3c1da2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_498ac9d3-2f7e-422d-9cf6-e524fa67fefa\", \"df_results.csv\", 6044)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwCNg2lGRKKFKo1wQ6dudE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20f500e38ccc4adb97613a447951789d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcf8aa9fb26148febf96cb467592297e",
              "IPY_MODEL_ff42af112f294ca1bc104454fc351f69",
              "IPY_MODEL_3d01ac28d3ba43409cccc869b682c6ee"
            ],
            "layout": "IPY_MODEL_dc74d12374b641caad89402e9d26877c"
          }
        },
        "fcf8aa9fb26148febf96cb467592297e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e6a0617b99a455789ea921268412e3a",
            "placeholder": "​",
            "style": "IPY_MODEL_c0f6ef6555d14164a5e4af9f6f62ae3d",
            "value": "model.safetensors: 100%"
          }
        },
        "ff42af112f294ca1bc104454fc351f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25030c2897624d4490922cda5351430b",
            "max": 6425529112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1120dc371a2b427fb6584510da134035",
            "value": 6425528500
          }
        },
        "3d01ac28d3ba43409cccc869b682c6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea4a6781d9d44303a9bec089c0eed203",
            "placeholder": "​",
            "style": "IPY_MODEL_e122af4ae199455ab3d6053ddf5a3803",
            "value": " 6.43G/6.43G [01:09&lt;00:00, 199MB/s]"
          }
        },
        "dc74d12374b641caad89402e9d26877c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e6a0617b99a455789ea921268412e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f6ef6555d14164a5e4af9f6f62ae3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25030c2897624d4490922cda5351430b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1120dc371a2b427fb6584510da134035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea4a6781d9d44303a9bec089c0eed203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e122af4ae199455ab3d6053ddf5a3803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bcde2fb21064fe6a5453775af590933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e64e947b85b4aab9700308ebc0231f1",
              "IPY_MODEL_b5a8008648054c3c9c6da8ee9b4a74b3",
              "IPY_MODEL_67c7d7acc2754e8f81e2fcf90f55c4bb"
            ],
            "layout": "IPY_MODEL_af4e1ded3906453888c235da83191166"
          }
        },
        "8e64e947b85b4aab9700308ebc0231f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2478558c01b04188aafcdb52a4a573de",
            "placeholder": "​",
            "style": "IPY_MODEL_9eebd25e88974fe4bbf4b1d6083334a8",
            "value": "generation_config.json: 100%"
          }
        },
        "b5a8008648054c3c9c6da8ee9b4a74b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72387e3bd8624018be1e6fef764ba99f",
            "max": 234,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ec71d9a13514d639d94638acb4155e6",
            "value": 234
          }
        },
        "67c7d7acc2754e8f81e2fcf90f55c4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3baae1d414bd47a6a8f994ef9d5c269d",
            "placeholder": "​",
            "style": "IPY_MODEL_f8fc6461f40b42fd87d2a3358442a07e",
            "value": " 234/234 [00:00&lt;00:00, 16.8kB/s]"
          }
        },
        "af4e1ded3906453888c235da83191166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2478558c01b04188aafcdb52a4a573de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eebd25e88974fe4bbf4b1d6083334a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72387e3bd8624018be1e6fef764ba99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec71d9a13514d639d94638acb4155e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3baae1d414bd47a6a8f994ef9d5c269d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8fc6461f40b42fd87d2a3358442a07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}